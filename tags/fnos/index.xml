<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>fnOS on 喵ฅ^•ﻌ•^ฅ</title>
    <link>https://ruohai.wang/tags/fnos/</link>
    <description>Recent content in fnOS on 喵ฅ^•ﻌ•^ฅ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 13 Jun 2025 08:09:38 +0800</lastBuildDate><atom:link href="https://ruohai.wang/tags/fnos/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>fnOS使用nginx反代后无法访问的问题解决</title>
      <link>https://ruohai.wang/202506/fnos-nginx-reverse-proxy-config/</link>
      <pubDate>Fri, 13 Jun 2025 08:09:38 +0800</pubDate>
      
      <guid>https://ruohai.wang/202506/fnos-nginx-reverse-proxy-config/</guid>
      <description>fnos自带了ddns，配置方便，用起来也很方便。但我不像把fnos暴露到公网，而且我在局域网中已经部署了一个虚拟机专门用来做反代，所以想把fnos的入口也整合到一起。
但是fnos用nginx做了反代以后会无法访问，具体表现就是卡在【加载中】最后显示断开连接。
在网上搜索一番，看了官方论坛里的一些答疑，确认了是我的nginx配置中没有开启websocket的支持，只需要加上以下三行配置即可
proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &amp;#34;upgrade&amp;#34;; 配置说明 proxy_http_version 1.1;
WebSocket 协议依赖 HTTP/1.1 的 Upgrade 机制，因此需要明确指定 Nginx 使用 HTTP/1.1 协议与后端服务器通信。 默认情况下，Nginx 可能使用 HTTP/1.0，而 HTTP/1.0 不支持 Upgrade 头，因此必须设置为 1.1。 proxy_set_header Upgrade $http_upgrade;
用途：将客户端发送的 Upgrade 请求头（如 websocket）转发到后端服务器。 作用：WebSocket 协议的建立需要客户端发送 Upgrade: websocket 请求头，告诉服务器需要切换协议。Nginx 作为反向代理，必须将这个头传递给后端，确保后端知道客户端请求的是 WebSocket 协议。 $http_upgrade 是 Nginx 的变量，表示客户端请求中 Upgrade 头的值。如果没有 Upgrade 头，$http_upgrade 为空。 proxy_set_header Connection &amp;quot;upgrade&amp;quot;;
用途：将客户端的 Connection 请求头设置为 upgrade，并转发到后端服务器。 作用：WebSocket 协议要求 HTTP 请求头中的 Connection: Upgrade 来指示协议切换。Nginx 需要显式设置 Connection 头为 &amp;quot;upgrade&amp;quot;，以确保后端服务器能够正确识别 WebSocket 请求。 注意：这里直接设置为 &amp;quot;upgrade&amp;quot;，而非使用变量 $http_connection，是因为 WebSocket 协议明确要求 Connection 头值为 upgrade，无需动态获取。 完整的nginx配置如下：</description>
    </item>
    
    <item>
      <title>记录一次RAID5 &#43; BTRFS的文件损坏与恢复</title>
      <link>https://ruohai.wang/202506/recover-file-damage/</link>
      <pubDate>Wed, 11 Jun 2025 20:42:41 +0800</pubDate>
      
      <guid>https://ruohai.wang/202506/recover-file-damage/</guid>
      <description>1. 前言 上个月的时候我写了一篇《在飞牛fnOS上手动进行文件完整性校验》，当时纯粹是当作了解学习一个新的知识点并顺手做一个记录，但没有想到这么快就派上了用场，因为在不到一个月之后的6月9号，我在自己nas的raid5 + btrfs的存储空间进行btrfs scrub的时候，竟然报了6000多个uncorrectable errors（截图的时候是5000多个，等扫描完了达到6000多个）。
这可如何是好啊！虽然折腾nas折腾linux也有小2年，但这应该是我第一次发现了文件损坏的情况。以前肯定也有，因为我用的硬盘都是二手拆机硬盘，质量参差不齐，只要系统（黑群或者fnos）没有报异常，就当没有。
不过遇事不能慌，要慢慢理顺逻辑，搞清楚这6000多个无法修正的错误代表什么意思，然后想办法修复它们。
在写下这篇文章的时候，我已经完成了这次文件损坏的恢复，因为万幸我有一个备份nas，最后从备份中恢复了损坏的300多个文件。
这篇文章简单做个记录，希望对你也有一些帮助。
tips：要明确一点，btrfs scrub扫出来的uncorrectable errors就是无法修正的错误，没法通过mdadm raid进行恢复，因为mdadm是硬件冗余而不是备份，不是备份，不是备份。如果没有备份，那就没办法恢复了。 2. 事件回溯 我的NAS配置
我的nas的软硬件环境如下：
软件：系统是fnos，存储方案是mdadm + lvm + btrfs 硬件：基于intel h55主板diy的主机，有6个原生sata接口。一开始用其中5个sata口组建了5盘位raid5，后来换机箱，用了一块pcie转5 sata转接板实现了6+5一共11个sata接口，这个5盘位raid5阵列，2个盘插在原生sata口，3个盘插在转接的sata口。 中间发生了什么
在5月14日的时候执行btrfs scrub提示no errors found，所以此时没有文件损坏。 在6月7号时更换了机箱，从普通机箱换成了8盘位nas机箱，更换完成后成功点亮系统无异常。 点亮系统后，对5盘位的raid5阵列进行了更换其中一块硬盘的操作，也就是【在web控制台停用一块硬盘 + raid5降级运行 + 添加一个新硬盘 + raid5阵列重建】，整个过程顺利完成，无异常报错。 对重建后的这个5盘位raid5阵列进行btrfs scrub操作，出现大量的uncorrectable errors报错。 对这个raid5阵列进行mdadm --check操作，验证完成后无报错无异常。 fnos的web控制台中查看存储空间，显示正常。（图片为示意图） tips：不管是群晖还是fnos，在web控制台中显示的阵列是否健康或者正常，针对的都是用mdadm创建的这个raid是否健康，mdadm --check校验的内容是条带化数据与奇偶校验块是否一致。这个健康或者正常，无法代表底层文件系统（这里是btrfs）上的文件是否出现了静默损坏或一致性异常。 异常原因分析
根据以上回溯信息，大致可以判断，应该是更换机箱 + raid5换盘重建的过程中出现了问题，但无法确定具体是哪一步、哪一个硬件的问题。也许是pcie转sata口转接板有问题？也许是用拆机二手盘组raid5在重建的时候碰到坏道导致的文件损坏？也许是因为内存不带ecc？也许是新机箱的电气屏蔽不好？也许是老旧的atx电源质量太差稳定性不足？
但这些都不是当下的重点了，现在的重点是要想办法处理btrfs文件系统的6000多个errors。
3. 文件恢复 虽然btrfs scrub明确提示了有6000多个errors，但是没有明确给出重要信息：
有没有文件损坏 有多少个文件损坏 是哪些文件损坏 因为6000多个无法修复的错误可能只涉及几个文件，也可能涉及更多文件，具体取决于受损数据块属于哪些文件。
我有另一个冷备nas上有备份，但这个备份上一次同步已经是2个月之前，没法简单的直接还原。我需要一份准确的损坏的文件清单，然后在备份nas中找到这份清单上的文件进行恢复。
所以，这次文件恢复的最重要、最核心的任务就明确了：拿到损坏文件的清单。
tips：因为我的raid5阵列显示健康度clean没有问题，mdadm --check也没有报异常，所以我没有着急备份阵列（或者说备份阵列中的数据）。但我依然建议你如果碰到这种情况，优先进行备份。 方法一：查看内核日志（不推荐）
btrfs文件系统在进行scrub的过程中，碰到校验和错误，理论上异常信息中会包含对应的文件信息。
dmesg | grep -i btrfs 输出的日志信息大致如下：</description>
    </item>
    
    <item>
      <title>飞牛fnOS下DLNA服务的防火墙设置</title>
      <link>https://ruohai.wang/202505/fnos-minidlna-filewall-setting/</link>
      <pubDate>Thu, 22 May 2025 15:50:26 +0800</pubDate>
      
      <guid>https://ruohai.wang/202505/fnos-minidlna-filewall-setting/</guid>
      <description>前言 之前我在debian基础上手搓minidlna服务，只需要在ufw中显式添加一条ufw allow proto tcp from 192.168.1.0/24 to any port 8200后，局域网内的设备（比如windows电脑文件管理器、安卓手机上的vlc软件中）都会自动出现dlna服务。
但是在fnos中，如果防火墙设置如下：
不勾选【局域网：默认允许访问】 不匹配入站规时禁止访问 然后新增一个DLNA默认端口8200的放行规则 理论上应该和debian + ufw + dlna这套手搓方案一样，局域网的机器就可以收到dlna服务了。
但实际上局域网内设备无法收到dlna的服务，或者严谨一点说，没有收到dlna的广播。
所以很长一段时间我都在pve上弄了一个单独的vm跑minidlna，而不是把dlna服务整合到fnos上。
不过，昨天把这个问题请教了ai以后，终于知道原因了。
问题原因 简单来讲，dlna除了用到8200/tcp，还需要用到1900/udp，所以只放行8200/tcp是不行的。
以下是ai提供的回答（其中48200端口是我自定义的dlna端口号）
到这里，又会出现第二个问题：
那为什么用debian + ufw的时候，只需要放行8200/tcp就可以了，在fnos下却还需要显示声明1900/udp呢？
答案是ufw默认会允许局域网流量！
以下是ai提供的表格
所以，最后的解决方案，就是在fnos的防火墙入站规则中，添加一条1900/udp即可。
🎉
喝杯奶茶 </description>
    </item>
    
    <item>
      <title>在飞牛fnOS上手动进行文件完整性校验</title>
      <link>https://ruohai.wang/202505/check-data-consistency/</link>
      <pubDate>Wed, 14 May 2025 00:03:33 +0800</pubDate>
      
      <guid>https://ruohai.wang/202505/check-data-consistency/</guid>
      <description>前言 玩过群晖的朋友都知道群晖的存储池在创建和使用的过程中经常能看到正在校验文件的一致性这个提示，虽然不知道底层原理但看着就让人很安心，让用户觉得文件绝对不会损坏。
fnos的存储空间的实现方案和群晖一样，都是mdadm + lvm + btrfs，但是fnos的存储空间在创建和使用的过程中，完全看不到校验文件一致性的提示。这不由得让我担心我的文件是否会出现什么宇宙射线引起的比特翻转但系统又没有定期做scrub导致文件损坏。
那fnos到底有没有进行文件一致性校验，我不知道。
但是我们可以手动进行文件校验。
文件校验 fnos的存储空间实现方案是mdadm + lvm + btfs，在2025年5月的更新中加入了ext4的支持，但我这里依然以btrfs为例。
要进行的文件校验分成两个部分：raid阵列的一致性校验 &amp;amp; 文件数据的一致性校验。
以下由ai提供的图表
那接下去就手动开始进行文件校验吧。
btrfs
以fnos的存储空间为例，一般挂载点都是/vol1，那btrfs的文件校验是以挂载点为目标
# 对存储空间1启动文件校验 sudo btrfs scrub start /vol1 # 查看校验的进度 sudo btrfs scrub status /vol1 校验完整后，可以看到输出结果如下
UUID: 6412be10-xxxx-xxxx-xxxx-xxxxxxxxxxxx Scrub started: Tue May 13 23:22:05 2025 Status: finished Duration: 1:15:23 Total to scrub: 514.29GiB Rate: 116.43MiB/s Error summary: no errors found mdadm
先确认系统的阵列状态
cat /proc/mdstat 输出结果示意如下
Personalities : [raid6] [raid5] [raid4] [raid1] [linear] [raid0] [raid10] md1 : active raid1 sde1[0] 156156928 blocks super 1.</description>
    </item>
    
    <item>
      <title>飞牛系统（fnOS）开启OVS后导致网络掉线的问题解决</title>
      <link>https://ruohai.wang/202504/fnos-ovs-makes-network-down/</link>
      <pubDate>Mon, 07 Apr 2025 14:57:26 +0800</pubDate>
      
      <guid>https://ruohai.wang/202504/fnos-ovs-makes-network-down/</guid>
      <description>fnos引入虚拟机系统以后在网络设置中添加了一个【启用OVS】的功能，大致是用openswitch工具来创建和管理虚拟网桥之类。
这个入口和编辑网络的入口放在一起，太过于明显，而且没有防呆设置，所以很容易误点导致开启了ovs网络。
当前fnos还处在快速开发期，存在很多bug，这个【误操作开启ovs网络】以后，很容易导致fnos出现网络问题，问题大致如下：
丢失公网，也就是所有公网服务掉线（比如ddns内网穿透、系统更新、数据刮削等等所有需要联网的服务），但局域网可以正常访问。 在上述【问题1】情况下，在【网络设置】中调整网络配置，会导致局域网也掉线，也就是无法访问fnos的网页，至此fnos公网和局域网的全部服务掉线，但重启机器后可以恢复到【问题1】 在上述【问题2】的情况下，如果继续在【网络设置】中开关OVS，不仅会导致fnos网络掉线，还会导致重启后也无法恢复网络，至此，只能通过将机器接上键盘和显示器，通过控制台访问 此处不深究是什么原因导致的网络崩溃，只说如何快速恢复网络：
将fnos主机接上显示器和键盘，通过控制台输入账户密码登录以后 用ovs-vsctl工具删除网桥即可 # 查看当前的网桥 sudo ovs-vsctl list-br # 删除网桥 sudo ovs-vsctl del-br &amp;lt;br-name&amp;gt; 重启网络或者重启系统 # 重启网络 sudo systemctl restart NetworkManager.service # 重启系统 sudo reboot 到这里网络就恢复正常了。
喝杯奶茶 </description>
    </item>
    
    <item>
      <title>飞牛系统fnos存储空间扩容/mdadm &#43; lvm &#43; btrfs方案的磁盘扩容</title>
      <link>https://ruohai.wang/202503/fnos-expand-vol-size/</link>
      <pubDate>Sat, 01 Mar 2025 22:17:00 +0800</pubDate>
      
      <guid>https://ruohai.wang/202503/fnos-expand-vol-size/</guid>
      <description>前言 飞牛系统的存储空间用的mdadm + lvm + btrfs的方案（群晖也是这个方案），如果想要扩容会比较麻烦，因为嵌套太多层了，不能像ext4文件系统那样用fdisk先d再n然后w最后resize2fs就行了，而且飞牛系统fnos截至目前还没有在网页控制台上添加扩容的功能。
自己盲操一遍发现比预想的复杂，不过最后还是在网上找到了别人的教程，所以写篇博客记录一下。
注意：操作有风险，记得给系统做快照、备份
操作 磁盘扩容的第一步肯定是换更大的硬盘或者在虚拟机host（pve或者esxi）中调整虚拟磁盘的大小。
这里以pve为例
在弹窗中调整大小
此时通过lsblk命令可以看到对应的磁盘/dev/sda已经变成从原来的10GB变成了15GB，但分区sda1、raid1阵列md0以及存储空间trim_xxxx的空间大小依然还是10GB。
接下去操作分成四步，依次是：
1. 扩容磁盘分区sda1
2. 扩容阵列md0
3. 扩容lvm卷trim-xxxx
4. 扩容存储空间vol1
tips: 如果是换更大容量的硬盘，可以直接换盘让阵列重建，或者用dd命令把旧盘克隆到新盘，后面的步骤就和下面的操作完全一样了。
第一步：扩容磁盘分区
分区扩容依然用到我们熟悉的fdisk，如果是gpt分区的话可以用gdisk，根据自己的情况使用。
操作步骤如图所示
步骤解析：
第一步是选择要扩容的磁盘，sudo fdisk /dev/sda，会要求输入密码认证，红字提示意思是分区大小和磁盘容量不符（废话我刚扩容了磁盘） 第二步是删除旧分区，敲字母d即可。这一步以及后面几步都无需担心数据丢失，因为操作只保留内存中，只要不保存都不会写入磁盘 第三步是创建新分区，敲字母n即可，然后会依次提示新分区的编号、新分区的起始扇区、新分区的结束扇区，都按回车用默认值 第四步出现红字提示说磁盘上有个旧分区表的签名，是否删除，敲字母n不删除 第五步，确认以上操作都没有问题，敲字母w保存本次操作 至此，分区扩容就完成了。我们用1sblk命令可以看到现在分区/dev/sda1的容量已经从10GB变成15GB惹。
tips：如果是多盘阵列，则需要依次扩容每个盘位的分区。
第二步：扩容阵列
首先确认要扩容的阵列的名字，我这里的阵列是md0。注意，在上一步中需要确认阵列中所有硬盘都已经完成了分区扩容。
可以先用mdadm -D /dev/md0命令查看阵列的状态
可以看到阵列md0当前的容量是10GB，状态是clean。
注意扩容阵列之前一定要留意阵列的状态，不要在非clean状态下扩容，说不定阵列会炸。
然后用命令mdadm --grow /dev/md0 --size=max进行扩容，会有提示阵列已经扩容到15GB，此时再用mdadm -D /dev/md0可以看到阵列已经变成15GB惹。
用lsblk再次确认阵列扩容结果
第三步：扩容lvm卷
用vgs命令查看当前的lvm卷的信息，可以看到还是10GB。
用pvresize /dev/md0命令将阵列md0的扩容信息同步给lvm
然后用vgs确认lvm卷的空间信息，可以看到对应的lvm卷已经多出了5GB的剩余空间。
接着继续扩容这个lvm卷，可以先用df -h命令获取这个lvm卷的完整路径名（注意不要忘记最后面的那个-0）
用以下命令扩容
sudo lvextend -l +100%FREE /dev/mapper/trim_ec850a20_129d_4f06_ad54_21eec30762f9-0 ## 成功提示 Size of logical volume trim_ec850a20_129d_4f06_ad54_21eec30762f9/0 changed from &amp;lt;9.</description>
    </item>
    
    <item>
      <title>用终端修改飞牛系统（fnOS）的IP地址</title>
      <link>https://ruohai.wang/202502/fnos-edit-ip-via-terminal/</link>
      <pubDate>Fri, 28 Feb 2025 21:55:14 +0800</pubDate>
      
      <guid>https://ruohai.wang/202502/fnos-edit-ip-via-terminal/</guid>
      <description>fnos的网络配置文件的路径有别于debian官方，不是/etc/network/interfaces，也不是/etc/netplan/xxxx.yaml。
fnos的网络配置文件正确路径是/etc/NetworkManager/system-connections/xxxx.nmconnection。
用nano/vim编辑以上文件即可，修改后通过systemctl restart NetworkManager.service重启网络服务后生效。
参考文章 飞牛fnOS中如何通过命令行修改IP地址~关键时刻可能会成为你的救命稻草~ 喝杯奶茶 </description>
    </item>
    
  </channel>
</rss>
