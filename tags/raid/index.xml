<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RAID on 喵ฅ^•ﻌ•^ฅ</title>
    <link>https://ruohai.wang/tags/raid/</link>
    <description>Recent content in RAID on 喵ฅ^•ﻌ•^ฅ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 29 Jul 2025 16:21:29 +0800</lastBuildDate><atom:link href="https://ruohai.wang/tags/raid/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>飞牛fnOS突然掉电导致RAID5存储空间无法识别的问题解决</title>
      <link>https://ruohai.wang/202507/fnos-raid5-unrecognize-repair/</link>
      <pubDate>Tue, 29 Jul 2025 16:21:29 +0800</pubDate>
      
      <guid>https://ruohai.wang/202507/fnos-raid5-unrecognize-repair/</guid>
      <description>前言 nas玩家迟早都会碰到的一个问题：因为突然断电，阵列无法识别了。
今天我就很不幸的遇到了，但幸运的是，在grok ai的帮助下，我最后成功的把阵列救了回来。
在这里做个记录，希望对你也有帮助。
注意事项：如无十分把握，请务必提前用dd命令给硬盘做镜像，避免raid修复失败导致数据彻底报废
问题描述 我用的nas系统是fnos，存储空间是3盘位raid5。因为一次意外断电，导致raid5阵列无法被fnos系统识别。
表现症状是：
在【硬盘信息】中可以看到硬盘，但都显示【未识别】或【未挂载】，以及【文件系统未知】，示意图如下。 在【存储空间】中无任何显示
ssh访问fnos，查看阵列的当前状态信息
一：用lsblk查看硬盘信息，确认阵列名称为/dev/md125。因为系统会尝试识别raid但最后识别失败，所以会出现2种结果。
# 系统识别raid有3个盘 sdh 8:112 0 232.9G 0 disk └─sdh1 8:113 0 232.9G 0 part └─md125 9:125 0 0B 0 md sdj 8:144 0 232.9G 0 disk └─sdj1 8:145 0 232.9G 0 part └─md125 9:125 0 0B 0 md sdk 8:160 0 232.9G 0 disk └─sdk1 8:161 0 232.9G 0 part └─md125 9:125 0 0B 0 md # 系统判定2个盘掉线，只有1个盘在线 sdh 8:112 0 232.</description>
    </item>
    
    <item>
      <title>记录一次RAID5 &#43; BTRFS的文件损坏与恢复</title>
      <link>https://ruohai.wang/202506/recover-file-damage/</link>
      <pubDate>Wed, 11 Jun 2025 20:42:41 +0800</pubDate>
      
      <guid>https://ruohai.wang/202506/recover-file-damage/</guid>
      <description>1. 前言 上个月的时候我写了一篇《在飞牛fnOS上手动进行文件完整性校验》，当时纯粹是当作了解学习一个新的知识点并顺手做一个记录，但没有想到这么快就派上了用场，因为在不到一个月之后的6月9号，我在自己nas的raid5 + btrfs的存储空间进行btrfs scrub的时候，竟然报了6000多个uncorrectable errors（截图的时候是5000多个，等扫描完了达到6000多个）。
这可如何是好啊！虽然折腾nas折腾linux也有小2年，但这应该是我第一次发现了文件损坏的情况。以前肯定也有，因为我用的硬盘都是二手拆机硬盘，质量参差不齐，只要系统（黑群或者fnos）没有报异常，就当没有。
不过遇事不能慌，要慢慢理顺逻辑，搞清楚这6000多个无法修正的错误代表什么意思，然后想办法修复它们。
在写下这篇文章的时候，我已经完成了这次文件损坏的恢复，因为万幸我有一个备份nas，最后从备份中恢复了损坏的300多个文件。
这篇文章简单做个记录，希望对你也有一些帮助。
tips：要明确一点，btrfs scrub扫出来的uncorrectable errors就是无法修正的错误，没法通过mdadm raid进行恢复，因为mdadm是硬件冗余而不是备份，不是备份，不是备份。如果没有备份，那就没办法恢复了。 2. 事件回溯 我的NAS配置
我的nas的软硬件环境如下：
软件：系统是fnos，存储方案是mdadm + lvm + btrfs 硬件：基于intel h55主板diy的主机，有6个原生sata接口。一开始用其中5个sata口组建了5盘位raid5，后来换机箱，用了一块pcie转5 sata转接板实现了6+5一共11个sata接口，这个5盘位raid5阵列，2个盘插在原生sata口，3个盘插在转接的sata口。 中间发生了什么
在5月14日的时候执行btrfs scrub提示no errors found，所以此时没有文件损坏。 在6月7号时更换了机箱，从普通机箱换成了8盘位nas机箱，更换完成后成功点亮系统无异常。 点亮系统后，对5盘位的raid5阵列进行了更换其中一块硬盘的操作，也就是【在web控制台停用一块硬盘 + raid5降级运行 + 添加一个新硬盘 + raid5阵列重建】，整个过程顺利完成，无异常报错。 对重建后的这个5盘位raid5阵列进行btrfs scrub操作，出现大量的uncorrectable errors报错。 对这个raid5阵列进行mdadm --check操作，验证完成后无报错无异常。 fnos的web控制台中查看存储空间，显示正常。（图片为示意图） tips：不管是群晖还是fnos，在web控制台中显示的阵列是否健康或者正常，针对的都是用mdadm创建的这个raid是否健康，mdadm --check校验的内容是条带化数据与奇偶校验块是否一致。这个健康或者正常，无法代表底层文件系统（这里是btrfs）上的文件是否出现了静默损坏或一致性异常。 异常原因分析
根据以上回溯信息，大致可以判断，应该是更换机箱 + raid5换盘重建的过程中出现了问题，但无法确定具体是哪一步、哪一个硬件的问题。也许是pcie转sata口转接板有问题？也许是用拆机二手盘组raid5在重建的时候碰到坏道导致的文件损坏？也许是因为内存不带ecc？也许是新机箱的电气屏蔽不好？也许是老旧的atx电源质量太差稳定性不足？
但这些都不是当下的重点了，现在的重点是要想办法处理btrfs文件系统的6000多个errors。
3. 文件恢复 虽然btrfs scrub明确提示了有6000多个errors，但是没有明确给出重要信息：
有没有文件损坏 有多少个文件损坏 是哪些文件损坏 因为6000多个无法修复的错误可能只涉及几个文件，也可能涉及更多文件，具体取决于受损数据块属于哪些文件。
我有另一个冷备nas上有备份，但这个备份上一次同步已经是2个月之前，没法简单的直接还原。我需要一份准确的损坏的文件清单，然后在备份nas中找到这份清单上的文件进行恢复。
所以，这次文件恢复的最重要、最核心的任务就明确了：拿到损坏文件的清单。
tips：因为我的raid5阵列显示健康度clean没有问题，mdadm --check也没有报异常，所以我没有着急备份阵列（或者说备份阵列中的数据）。但我依然建议你如果碰到这种情况，优先进行备份。 方法一：查看内核日志（不推荐）
btrfs文件系统在进行scrub的过程中，碰到校验和错误，理论上异常信息中会包含对应的文件信息。
dmesg | grep -i btrfs 输出的日志信息大致如下：</description>
    </item>
    
  </channel>
</rss>
