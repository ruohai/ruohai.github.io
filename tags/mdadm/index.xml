<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mdadm on 喵ฅ^•ﻌ•^ฅ</title>
    <link>https://ruohai.wang/tags/mdadm/</link>
    <description>Recent content in mdadm on 喵ฅ^•ﻌ•^ฅ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 14 Nov 2023 12:44:00 +0800</lastBuildDate><atom:link href="https://ruohai.wang/tags/mdadm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>往mdadm创建的raid5阵列中添加新硬盘以扩容</title>
      <link>https://ruohai.wang/202311/mdadm-add-new-hdd-to-raid5-arrary/</link>
      <pubDate>Tue, 14 Nov 2023 12:44:00 +0800</pubDate>
      
      <guid>https://ruohai.wang/202311/mdadm-add-new-hdd-to-raid5-arrary/</guid>
      <description>前言 之前手上只有4块hdd，虽然硬盘柜有5个盘位，但只是用4盘组了raid5。
raid5用了一阵以后，觉得还行，就想着把剩下的一个空盘位利用起来，所以又从pdd入手了好几片500GB的拆机硬盘。
硬盘到手了，照例又是查smart信息、全盘扫坏道。几块西数的是清零盘，但没有扫出坏道；几块希捷的通电时间惊人，一块3万小时，一块6万小时，但没有扫出坏道。
也就20rmb一片，既然没坏道，那就凑合着用吧。
raid5扩容 先描述下本次我想要实现的需求：有一个正在运行的四盘位raid5阵列，添加一个块同容量的硬盘，从四盘位扩展为五盘位。
有需求了就找解决方案，搜关键字mdadm、raid5、添加硬盘，但没有搜到什么靠谱可用的方案，那就自己摸索吧。
已有raid5阵列要添加硬盘，有两个方案：
方案一： 新建raid5
这个方案很傻瓜，但是非常的安全而且非常的效率，可行度100%。前提是你手上有很多备用硬盘，或者有一个大容量硬盘做数据中转。
如果手上硬盘数量足够多，那就保持旧四盘位raid不动，新组装一个五盘位raid5，然后直接从旧raid拷数据到新raid。整个任务所耗费的时间由两个raid的硬盘读写速度和局域网带宽决定。
如果手上硬盘不够用来组新raid，那先从旧raid把数据导出一份备份，然后停用四盘位旧raid + 加硬盘 + 创建五盘位新raid，最后把备份的数据再导入回去。
方案二：旧raid5扩容
这个方案有一定的风险性，操作前最好把数据备份一下。
旧四盘位的raid5，添加一块硬盘。假定新添加的硬盘是/dev/sdf。
第一步：清除分区数据。
因为我的旧raid是清除了分区直接裸盘组raid，所以新硬盘也先清除分区信息。如果你的raid是先分区然后以分区为单位组装的raid，那新硬盘也记得同样先分区。
# 查看分区信息 wipefs /dev/sdf # 清除分区信息 wipefs -a -f /dev/sdf 第二步：添加硬盘
新盘添加进阵列。假定阵列名为/dev/md0。
mdadm --manage /dev/md0 --add /dev/sdf 添加完成后，查看一下阵列信息。
mdadm -D /dev/md0 此时可以看到阵列的硬盘数量从4变成了5。但是新加入的硬盘并没有真正的融入进去，它的角色是hot spare热备盘，也就是当四盘raid中出现一块坏盘时，这块新盘会自动替上去重建raid修复数据。
第三步：重塑（reshape）
我的目的并不是给四盘位raid5添加一块热备盘（hot spare），我是想把新盘加进存储池扩充容量。
指定raid5的硬盘数量为5
mdadm --grow /dev/md0 --raid-devices=5 命令执行完成后，查看一下阵列信息
mdadm -D /dev/md0 此时就可以看到，存储池的硬盘从4块变成了5块，新的raid开始重塑（reshaping）。
整个重塑过程会非常非常非常的漫长，这个时间单位以小时甚至以天计算，请耐心等待mdadm完成任务。
第四步：调整阵列容量
在经历了漫长的重塑（reshape）以后，进度条终于到了100%，可以进行下一步操作了。
把raid容量调整为最大！
mdadm --grow /dev/md0 --size=max 查看阵列当前状态，会看到当前是resyncing(同步)。
mdadm --detail /dev/md0 然后耐心等待mdadm对阵列进行同步（resyncing），这个过程比重塑（reshape）快很多。</description>
    </item>
    
    <item>
      <title>Linux下迁移mdadm软raid</title>
      <link>https://ruohai.wang/202311/mdadm-auto-mount-raid/</link>
      <pubDate>Fri, 10 Nov 2023 18:03:30 +0800</pubDate>
      
      <guid>https://ruohai.wang/202311/mdadm-auto-mount-raid/</guid>
      <description>当在A系统下用mdadm组装的软raid阵列，如果全盘迁移到B系统时，如何正确的重新组装并识别。
有两种解决方案：
方案一：B系统重启 B系统先安装mdadm，然后关机，接着插上raid阵列的所有硬盘，最后B系统重启即可。B系统启动后会自动识别软raid阵列并正确组装，之后只需要动手完成挂载。
方案二：B系统不重启 现在A系统中卸载（umount）软raid阵列，然后把阵列的所有硬盘插到B系统主机。
这里又分两种情况。
先安装mdadm，然后raid硬盘都插入后，系统会自动识别raid并正确组装
先插入raid阵列所有硬盘，然后安装mdadm，此时系统无法自动识别raid，需要手动组装
设定raid阵列名为md0，4个阵列盘分为sdb、sdc、sdd、sde。执行以下命令，即可完成raid组装与识别，之后再手动完成挂载。
# 组装阵列，mdadm会自动识别raid类型 mdadm -A /dev/md0 /dev/sd[b-e] # 手动挂载 mount /dev/md0 /mnt/raid </description>
    </item>
    
    <item>
      <title>Linux下用mdadm创建软RAID以及避坑</title>
      <link>https://ruohai.wang/202310/mdadm-create-soft-raid-guide/</link>
      <pubDate>Tue, 31 Oct 2023 00:54:38 +0800</pubDate>
      
      <guid>https://ruohai.wang/202310/mdadm-create-soft-raid-guide/</guid>
      <description>前言 linux下组软raid用mdadm命令，multiple devices admin，多设备管理。
本文内容有二：
用mdadm创建raid 用mdadm创建raid时的一个大坑 大坑 先把大坑写前面。
用来创建raid的硬盘，不管是新盘还是旧盘，在linux中挂载以后，请先用wipefs命令清理硬盘上的分区表信息。
硬盘分区有两种，mbr和gpt，mbr可以直接裸盘不分区就创建raid，gpt必须要有分区才能创建raid。如果组raid的几块硬盘分区表信息不统一，虽然能成功创建raid，但是系统一重启就会掉raid，也就是raid消失了。没错，系统一重启raid就没了。如果你已经在这个raid上导入数据，这个坑肯定能惊出你一头汗。
所以拿到硬盘以后，最好是用wipefs直接清空分区表。也有人说用fdisk，但实际fdisk只能处理mbr分区表，需要用升级版gdisk才能处理gpt分区表。
假定磁盘是/dev/sdb
# 查看分区表 wipefs /dev/sdb 删除分区表所有信息
wipefs -a -f /dev/sdb 分区注意事项 创建raid之前，要简单了解一下mbr和gpt分区，避免后续再踩雷。
mbr：
不支持2TB以上容量的硬盘 可以不分区，以裸盘为单位创建raid 用fdisk处理分区 gpt：
支持2TB以上容量的硬盘 必须先分区，以分区为单位创建raid 用gdisk处理分区 创建磁盘阵列 我没有那么大容量的硬盘，而且系统也是legacy bios引导，所以这里选择mbr裸盘创建阵列。
第一步：查看分区表
先确认前面已经用wipefs清空了旧硬盘上的分区表信息。
# 查看分区表 wipefs /dev/sdb # 删除分区表 wipefs -a -f /dev/sdb 注意：如果是gpt分区，注意要用gdisk先分区，/dev/sdb1和/dev/sdc1，下面创建raid的时候也是用/dev/sdb1和/dev/sdc1。
第二步：创建raid0
mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc /dev/md0 ： 指创建的磁盘阵列的名称，如果有多个raid，可以按顺序往下编，比如/dev/md1 level=0 ：指raid0，还有1、5、10等等，对应raid1、raid5、raid10 raid-devices=2 ：指阵列的磁盘数量，2块盘就是2 创建完成后，可以用下面的命令查看raid信息。
mdadm --detail /dev/md0 第三步：创建文件系统（格式化）
raid创建成功以后，需要格式化
mkfs.ext4 /dev/md0 注意ext4文件格式下，系统会预留5%的冗余空间，简单换算就是1TB就会保留50GB空间。</description>
    </item>
    
  </channel>
</rss>
