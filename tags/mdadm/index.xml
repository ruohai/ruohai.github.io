<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mdadm on 喵ฅ^•ﻌ•^ฅ</title>
    <link>https://ruohai.wang/tags/mdadm/</link>
    <description>Recent content in mdadm on 喵ฅ^•ﻌ•^ฅ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 01 Mar 2025 22:17:00 +0800</lastBuildDate><atom:link href="https://ruohai.wang/tags/mdadm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>飞牛系统fnos存储空间扩容/mdadm &#43; lvm &#43; btrfs方案的磁盘扩容</title>
      <link>https://ruohai.wang/202503/fnos-expand-vol-size/</link>
      <pubDate>Sat, 01 Mar 2025 22:17:00 +0800</pubDate>
      
      <guid>https://ruohai.wang/202503/fnos-expand-vol-size/</guid>
      <description>前言 飞牛系统的存储空间用的mdadm + lvm + btrfs的方案（群晖也是这个方案），如果想要扩容会比较麻烦，因为嵌套太多层了，不能像ext4文件系统那样用fdisk先d再n然后w最后resize2fs就行了，而且飞牛系统fnos截至目前还没有在网页控制台上添加扩容的功能。
自己盲操一遍发现比预想的复杂，不过最后还是在网上找到了别人的教程，所以写篇博客记录一下。
注意：操作有风险，记得给系统做快照、备份
操作 磁盘扩容的第一步肯定是换更大的硬盘或者在虚拟机host（pve或者esxi）中调整虚拟磁盘的大小。
这里以pve为例
在弹窗中调整大小
此时通过lsblk命令可以看到对应的磁盘/dev/sda已经变成从原来的10GB变成了15GB，但分区sda1、raid1阵列md0以及存储空间trim_xxxx的空间大小依然还是10GB。
接下去操作分成四步，依次是：
1. 扩容磁盘分区sda1
2. 扩容阵列md0
3. 扩容lvm卷trim-xxxx
4. 扩容存储空间vol1
tips: 如果是换更大容量的硬盘，可以直接换盘让阵列重建，或者用dd命令把旧盘克隆到新盘，后面的步骤就和下面的操作完全一样了。
第一步：扩容磁盘分区
分区扩容依然用到我们熟悉的fdisk，如果是gpt分区的话可以用gdisk，根据自己的情况使用。
操作步骤如图所示
步骤解析：
第一步是选择要扩容的磁盘，sudo fdisk /dev/sda，会要求输入密码认证，红字提示意思是分区大小和磁盘容量不符（废话我刚扩容了磁盘） 第二步是删除旧分区，敲字母d即可。这一步以及后面几步都无需担心数据丢失，因为操作只保留内存中，只要不保存都不会写入磁盘 第三步是创建新分区，敲字母n即可，然后会依次提示新分区的编号、新分区的起始扇区、新分区的结束扇区，都按回车用默认值 第四步出现红字提示说磁盘上有个旧分区表的签名，是否删除，敲字母n不删除 第五步，确认以上操作都没有问题，敲字母w保存本次操作 至此，分区扩容就完成了。我们用1sblk命令可以看到现在分区/dev/sda1的容量已经从10GB变成15GB惹。
tips：如果是多盘阵列，则需要依次扩容每个盘位的分区。
第二步：扩容阵列
首先确认要扩容的阵列的名字，我这里的阵列是md0。注意，在上一步中需要确认阵列中所有硬盘都已经完成了分区扩容。
可以先用mdadm -D /dev/md0命令查看阵列的状态
可以看到阵列md0当前的容量是10GB，状态是clean。
注意扩容阵列之前一定要留意阵列的状态，不要在非clean状态下扩容，说不定阵列会炸。
然后用命令mdadm --grow /dev/md0 --size=max进行扩容，会有提示阵列已经扩容到15GB，此时再用mdadm -D /dev/md0可以看到阵列已经变成15GB惹。
用lsblk再次确认阵列扩容结果
第三步：扩容lvm卷
用vgs命令查看当前的lvm卷的信息，可以看到还是10GB。
用pvresize /dev/md0命令将阵列md0的扩容信息同步给lvm
然后用vgs确认lvm卷的空间信息，可以看到对应的lvm卷已经多出了5GB的剩余空间。
接着继续扩容这个lvm卷，可以先用df -h命令获取这个lvm卷的完整路径名（注意不要忘记最后面的那个-0）
用以下命令扩容
sudo lvextend -l +100%FREE /dev/mapper/trim_ec850a20_129d_4f06_ad54_21eec30762f9-0 ## 成功提示 Size of logical volume trim_ec850a20_129d_4f06_ad54_21eec30762f9/0 changed from &amp;lt;9.</description>
    </item>
    
    <item>
      <title>往mdadm创建的raid5阵列中添加新硬盘以扩容</title>
      <link>https://ruohai.wang/202311/mdadm-add-new-hdd-to-raid5-arrary/</link>
      <pubDate>Tue, 14 Nov 2023 12:44:00 +0800</pubDate>
      
      <guid>https://ruohai.wang/202311/mdadm-add-new-hdd-to-raid5-arrary/</guid>
      <description>前言 之前手上只有4块hdd，虽然硬盘柜有5个盘位，但只是用4盘组了raid5。
raid5用了一阵以后，觉得还行，就想着把剩下的一个空盘位利用起来，所以又从pdd入手了好几片500GB的拆机硬盘。
硬盘到手了，照例又是查smart信息、全盘扫坏道。几块西数的是清零盘，但没有扫出坏道；几块希捷的通电时间惊人，一块3万小时，一块6万小时，但没有扫出坏道。
也就20rmb一片，既然没坏道，那就凑合着用吧。
raid5扩容 先描述下本次我想要实现的需求：有一个正在运行的四盘位raid5阵列，添加一个块同容量的硬盘，从四盘位扩展为五盘位。
有需求了就找解决方案，搜关键字mdadm、raid5、添加硬盘，但没有搜到什么靠谱可用的方案，那就自己摸索吧。
已有raid5阵列要添加硬盘，有两个方案：
方案一： 新建raid5
这个方案很傻瓜，但是非常的安全而且非常的效率，可行度100%。前提是你手上有很多备用硬盘，或者有一个大容量硬盘做数据中转。
如果手上硬盘数量足够多，那就保持旧四盘位raid不动，新组装一个五盘位raid5，然后直接从旧raid拷数据到新raid。整个任务所耗费的时间由两个raid的硬盘读写速度和局域网带宽决定。
如果手上硬盘不够用来组新raid，那先从旧raid把数据导出一份备份，然后停用四盘位旧raid + 加硬盘 + 创建五盘位新raid，最后把备份的数据再导入回去。
方案二：旧raid5扩容
这个方案有一定的风险性，操作前最好把数据备份一下。
旧四盘位的raid5，添加一块硬盘。假定新添加的硬盘是/dev/sdf。
第一步：清除分区数据。
因为我的旧raid是清除了分区直接裸盘组raid，所以新硬盘也先清除分区信息。如果你的raid是先分区然后以分区为单位组装的raid，那新硬盘也记得同样先分区。
# 查看分区信息 wipefs /dev/sdf # 清除分区信息 wipefs -a -f /dev/sdf 第二步：添加硬盘
新盘添加进阵列。假定阵列名为/dev/md0。
mdadm --manage /dev/md0 --add /dev/sdf 添加完成后，查看一下阵列信息。
mdadm -D /dev/md0 此时可以看到阵列的硬盘数量从4变成了5。但是新加入的硬盘并没有真正的融入进去，它的角色是hot spare热备盘，也就是当四盘raid中出现一块坏盘时，这块新盘会自动替上去重建raid修复数据。
第三步：重塑（reshape）
我的目的并不是给四盘位raid5添加一块热备盘（hot spare），我是想把新盘加进存储池扩充容量。
指定raid5的硬盘数量为5
mdadm --grow /dev/md0 --raid-devices=5 命令执行完成后，查看一下阵列信息
mdadm -D /dev/md0 此时就可以看到，存储池的硬盘从4块变成了5块，新的raid开始重塑（reshaping）。
整个重塑过程会非常非常非常的漫长，这个时间单位以小时甚至以天计算，请耐心等待mdadm完成任务。
第四步：调整阵列容量
在经历了漫长的重塑（reshape）以后，进度条终于到了100%，可以进行下一步操作了。
把raid容量调整为最大！
mdadm --grow /dev/md0 --size=max 查看阵列当前状态，会看到当前是resyncing(同步)。
mdadm --detail /dev/md0 然后耐心等待mdadm对阵列进行同步（resyncing），这个过程比重塑（reshape）快很多。</description>
    </item>
    
    <item>
      <title>Linux下迁移mdadm软raid</title>
      <link>https://ruohai.wang/202311/mdadm-auto-mount-raid/</link>
      <pubDate>Fri, 10 Nov 2023 18:03:30 +0800</pubDate>
      
      <guid>https://ruohai.wang/202311/mdadm-auto-mount-raid/</guid>
      <description>当在A系统下用mdadm组装的软raid阵列，如果全盘迁移到B系统时，如何正确的重新组装并识别。
有两种解决方案：
方案一：B系统重启 B系统先安装mdadm，然后关机，接着插上raid阵列的所有硬盘，最后B系统重启即可。B系统启动后会自动识别软raid阵列并正确组装，之后只需要动手完成挂载。
方案二：B系统不重启 现在A系统中卸载（umount）软raid阵列，然后把阵列的所有硬盘插到B系统主机。
这里又分两种情况。
先安装mdadm，然后raid硬盘都插入后，系统会自动识别raid并正确组装
先插入raid阵列所有硬盘，然后安装mdadm，此时系统无法自动识别raid，需要手动组装
设定raid阵列名为md0，4个阵列盘分为sdb、sdc、sdd、sde。执行以下命令，即可完成raid组装与识别，之后再手动完成挂载。
# 组装阵列，mdadm会自动识别raid类型 mdadm -A /dev/md0 /dev/sd[b-e] # 手动挂载 mount /dev/md0 /mnt/raid </description>
    </item>
    
    <item>
      <title>Linux下用mdadm创建软RAID以及避坑</title>
      <link>https://ruohai.wang/202310/mdadm-create-soft-raid-guide/</link>
      <pubDate>Tue, 31 Oct 2023 00:54:38 +0800</pubDate>
      
      <guid>https://ruohai.wang/202310/mdadm-create-soft-raid-guide/</guid>
      <description>前言 linux下组软raid用mdadm命令，multiple devices admin，多设备管理。
本文内容有二：
用mdadm创建raid 用mdadm创建raid时的一个大坑 大坑 先把大坑写前面。
用来创建raid的硬盘，不管是新盘还是旧盘，在linux中挂载以后，请先用wipefs命令清理硬盘上的分区表信息。
硬盘分区有两种，mbr和gpt，mbr可以直接裸盘不分区就创建raid，gpt必须要有分区才能创建raid。如果组raid的几块硬盘分区表信息不统一，虽然能成功创建raid，但是系统一重启就会掉raid，也就是raid消失了。没错，系统一重启raid就没了。如果你已经在这个raid上导入数据，这个坑肯定能惊出你一头汗。
所以拿到硬盘以后，最好是用wipefs直接清空分区表。也有人说用fdisk，但实际fdisk只能处理mbr分区表，需要用升级版gdisk才能处理gpt分区表。
假定磁盘是/dev/sdb
# 查看分区表 wipefs /dev/sdb 删除分区表所有信息
wipefs -a -f /dev/sdb 分区注意事项 创建raid之前，要简单了解一下mbr和gpt分区，避免后续再踩雷。
mbr：
不支持2TB以上容量的硬盘 可以不分区，以裸盘为单位创建raid 用fdisk处理分区 gpt：
支持2TB以上容量的硬盘 必须先分区，以分区为单位创建raid 用gdisk处理分区 创建磁盘阵列 我没有那么大容量的硬盘，而且系统也是legacy bios引导，所以这里选择mbr裸盘创建阵列。
第一步：查看分区表
先确认前面已经用wipefs清空了旧硬盘上的分区表信息。
# 查看分区表 wipefs /dev/sdb # 删除分区表 wipefs -a -f /dev/sdb 注意：如果是gpt分区，注意要用gdisk先分区，/dev/sdb1和/dev/sdc1，下面创建raid的时候也是用/dev/sdb1和/dev/sdc1。
第二步：创建raid0
mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc /dev/md0 ： 指创建的磁盘阵列的名称，如果有多个raid，可以按顺序往下编，比如/dev/md1 level=0 ：指raid0，还有1、5、10等等，对应raid1、raid5、raid10 raid-devices=2 ：指阵列的磁盘数量，2块盘就是2 创建完成后，可以用下面的命令查看raid信息。
mdadm --detail /dev/md0 第三步：创建文件系统（格式化）
raid创建成功以后，需要格式化
mkfs.ext4 /dev/md0 注意ext4文件格式下，系统会预留5%的冗余空间，简单换算就是1TB就会保留50GB空间。</description>
    </item>
    
  </channel>
</rss>
