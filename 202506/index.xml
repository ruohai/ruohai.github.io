<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>202506s on 喵ฅ^•ﻌ•^ฅ</title>
    <link>https://ruohai.wang/202506/</link>
    <description>Recent content in 202506s on 喵ฅ^•ﻌ•^ฅ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 28 Jun 2025 20:59:48 +0800</lastBuildDate><atom:link href="https://ruohai.wang/202506/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>飞牛fnOS硬盘反复唤醒休眠的问题解决</title>
      <link>https://ruohai.wang/202506/fnos-hdd-auto-sleep-config/</link>
      <pubDate>Sat, 28 Jun 2025 20:59:48 +0800</pubDate>
      
      <guid>https://ruohai.wang/202506/fnos-hdd-auto-sleep-config/</guid>
      <description>先说一下我的fnos硬件方案：
pve host上虚拟机部署fnos 通过usb3.0接口外接一个硬盘柜 硬盘柜的固件默认10min无io会将硬盘休眠 在fnos中设置【外置硬盘】的休眠策略为【从不】 在以上的硬件方案下，在fnos的日志中会看到硬盘高频 &amp;amp; 反复出现硬盘休眠、硬盘唤醒的内容。
nas的硬盘要不要休眠这是一个许多人在争论的话题，有些人觉得功耗高所以选择没有io就要休眠，有些人觉得反复休眠影响硬盘寿命所以选择不休眠。但不管哪个方案下，这种几分钟就往复一次的休眠 + 唤醒都是无法接受的。
最近几个版本的fnos更新里都会提到硬盘休眠功能的bugfix和优化，但是因为我使用的硬盘柜的固件强制10min无io就自动休眠 &amp;amp; 无法修改，所以fnos暂时无法正确处理我这套硬件方案下的硬盘休眠问题。
那只能我们自己动手了！
linux下控制硬盘休眠最常用的工具就是hdparm了，直接ssh连上fnos
# 禁止休眠 sudo hdparm -S 0 /dev/sdf # 调整电源管理级别 sudo hdparm -B 254 /dev/sdf -S 0，表示禁用 standby 模式，硬盘将不会自动进入休眠状态 -B 254，将 APM 设置为最大性能模式，尽量减少电源管理干预 执行完成后，可以通过以下命令查看电源管理设置是否生效
sudo hdparm -I /dev/sdf | grep &amp;#39;Advanced power management level&amp;#39; 完成以上设置以后就静观其变了，看下fnos的日志中是否还会出现硬盘反复休眠 + 唤醒的情况。
如果fnos系统重启以后以上设置被重置，只需要写个bash脚本配合crontab定时执行即可。
喝杯奶茶 </description>
    </item>
    
    <item>
      <title>fnOS使用nginx反代后无法访问的问题解决</title>
      <link>https://ruohai.wang/202506/fnos-nginx-reverse-proxy-config/</link>
      <pubDate>Fri, 13 Jun 2025 08:09:38 +0800</pubDate>
      
      <guid>https://ruohai.wang/202506/fnos-nginx-reverse-proxy-config/</guid>
      <description>fnos自带了ddns，配置方便，用起来也很方便。但我不像把fnos暴露到公网，而且我在局域网中已经部署了一个虚拟机专门用来做反代，所以想把fnos的入口也整合到一起。
但是fnos用nginx做了反代以后会无法访问，具体表现就是卡在【加载中】最后显示断开连接。
在网上搜索一番，看了官方论坛里的一些答疑，确认了是我的nginx配置中没有开启websocket的支持，只需要加上以下三行配置即可
proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection &amp;#34;upgrade&amp;#34;; 配置说明 proxy_http_version 1.1;
WebSocket 协议依赖 HTTP/1.1 的 Upgrade 机制，因此需要明确指定 Nginx 使用 HTTP/1.1 协议与后端服务器通信。 默认情况下，Nginx 可能使用 HTTP/1.0，而 HTTP/1.0 不支持 Upgrade 头，因此必须设置为 1.1。 proxy_set_header Upgrade $http_upgrade;
用途：将客户端发送的 Upgrade 请求头（如 websocket）转发到后端服务器。 作用：WebSocket 协议的建立需要客户端发送 Upgrade: websocket 请求头，告诉服务器需要切换协议。Nginx 作为反向代理，必须将这个头传递给后端，确保后端知道客户端请求的是 WebSocket 协议。 $http_upgrade 是 Nginx 的变量，表示客户端请求中 Upgrade 头的值。如果没有 Upgrade 头，$http_upgrade 为空。 proxy_set_header Connection &amp;quot;upgrade&amp;quot;;
用途：将客户端的 Connection 请求头设置为 upgrade，并转发到后端服务器。 作用：WebSocket 协议要求 HTTP 请求头中的 Connection: Upgrade 来指示协议切换。Nginx 需要显式设置 Connection 头为 &amp;quot;upgrade&amp;quot;，以确保后端服务器能够正确识别 WebSocket 请求。 注意：这里直接设置为 &amp;quot;upgrade&amp;quot;，而非使用变量 $http_connection，是因为 WebSocket 协议明确要求 Connection 头值为 upgrade，无需动态获取。 完整的nginx配置如下：</description>
    </item>
    
    <item>
      <title>记录一次RAID5 &#43; BTRFS的文件损坏与恢复</title>
      <link>https://ruohai.wang/202506/recover-file-damage/</link>
      <pubDate>Wed, 11 Jun 2025 20:42:41 +0800</pubDate>
      
      <guid>https://ruohai.wang/202506/recover-file-damage/</guid>
      <description>1. 前言 上个月的时候我写了一篇《在飞牛fnOS上手动进行文件完整性校验》，当时纯粹是当作了解学习一个新的知识点并顺手做一个记录，但没有想到这么快就派上了用场，因为在不到一个月之后的6月9号，我在自己nas的raid5 + btrfs的存储空间进行btrfs scrub的时候，竟然报了6000多个uncorrectable errors（截图的时候是5000多个，等扫描完了达到6000多个）。
这可如何是好啊！虽然折腾nas折腾linux也有小2年，但这应该是我第一次发现了文件损坏的情况。以前肯定也有，因为我用的硬盘都是二手拆机硬盘，质量参差不齐，只要系统（黑群或者fnos）没有报异常，就当没有。
不过遇事不能慌，要慢慢理顺逻辑，搞清楚这6000多个无法修正的错误代表什么意思，然后想办法修复它们。
在写下这篇文章的时候，我已经完成了这次文件损坏的恢复，因为万幸我有一个备份nas，最后从备份中恢复了损坏的300多个文件。
这篇文章简单做个记录，希望对你也有一些帮助。
tips：要明确一点，btrfs scrub扫出来的uncorrectable errors就是无法修正的错误，没法通过mdadm raid进行恢复，因为mdadm是硬件冗余而不是备份，不是备份，不是备份。如果没有备份，那就没办法恢复了。 2. 事件回溯 我的NAS配置
我的nas的软硬件环境如下：
软件：系统是fnos，存储方案是mdadm + lvm + btrfs 硬件：基于intel h55主板diy的主机，有6个原生sata接口。一开始用其中5个sata口组建了5盘位raid5，后来换机箱，用了一块pcie转5 sata转接板实现了6+5一共11个sata接口，这个5盘位raid5阵列，2个盘插在原生sata口，3个盘插在转接的sata口。 中间发生了什么
在5月14日的时候执行btrfs scrub提示no errors found，所以此时没有文件损坏。 在6月7号时更换了机箱，从普通机箱换成了8盘位nas机箱，更换完成后成功点亮系统无异常。 点亮系统后，对5盘位的raid5阵列进行了更换其中一块硬盘的操作，也就是【在web控制台停用一块硬盘 + raid5降级运行 + 添加一个新硬盘 + raid5阵列重建】，整个过程顺利完成，无异常报错。 对重建后的这个5盘位raid5阵列进行btrfs scrub操作，出现大量的uncorrectable errors报错。 对这个raid5阵列进行mdadm --check操作，验证完成后无报错无异常。 fnos的web控制台中查看存储空间，显示正常。（图片为示意图） tips：不管是群晖还是fnos，在web控制台中显示的阵列是否健康或者正常，针对的都是用mdadm创建的这个raid是否健康，mdadm --check校验的内容是条带化数据与奇偶校验块是否一致。这个健康或者正常，无法代表底层文件系统（这里是btrfs）上的文件是否出现了静默损坏或一致性异常。 异常原因分析
根据以上回溯信息，大致可以判断，应该是更换机箱 + raid5换盘重建的过程中出现了问题，但无法确定具体是哪一步、哪一个硬件的问题。也许是pcie转sata口转接板有问题？也许是用拆机二手盘组raid5在重建的时候碰到坏道导致的文件损坏？也许是因为内存不带ecc？也许是新机箱的电气屏蔽不好？也许是老旧的atx电源质量太差稳定性不足？
但这些都不是当下的重点了，现在的重点是要想办法处理btrfs文件系统的6000多个errors。
3. 文件恢复 虽然btrfs scrub明确提示了有6000多个errors，但是没有明确给出重要信息：
有没有文件损坏 有多少个文件损坏 是哪些文件损坏 因为6000多个无法修复的错误可能只涉及几个文件，也可能涉及更多文件，具体取决于受损数据块属于哪些文件。
我有另一个冷备nas上有备份，但这个备份上一次同步已经是2个月之前，没法简单的直接还原。我需要一份准确的损坏的文件清单，然后在备份nas中找到这份清单上的文件进行恢复。
所以，这次文件恢复的最重要、最核心的任务就明确了：拿到损坏文件的清单。
tips：因为我的raid5阵列显示健康度clean没有问题，mdadm --check也没有报异常，所以我没有着急备份阵列（或者说备份阵列中的数据）。但我依然建议你如果碰到这种情况，优先进行备份。 方法一：查看内核日志（不推荐）
btrfs文件系统在进行scrub的过程中，碰到校验和错误，理论上异常信息中会包含对应的文件信息。
dmesg | grep -i btrfs 输出的日志信息大致如下：</description>
    </item>
    
  </channel>
</rss>
